{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flowing this \n",
    "# https://www.youtube.com/watch?v=nMwjgCchTJc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "using MLDatasets, Flux, Plots,  Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; … ;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], [5, 0, 4, 1, 9, 2, 1, 3, 1, 4  …  9, 2, 9, 5, 1, 8, 3, 5, 6, 8])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x, train_y = MNIST.traindata(Float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; … ;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], [7, 2, 1, 0, 4, 1, 4, 9, 5, 9  …  7, 8, 9, 0, 1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x, test_y = MNIST.testdata(Float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28, 60000)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAAgY0hSTQAAeiYAAICEAAD6AAAAgOgAAHUwAADqYAAAOpgAABdwnLpRPAAAAilJREFUaAW9wT+I1QUAB/DPq+8QgSU1uFTW0Guzg4ZL8KjZoYOWaqilpSWCOx0aczYSWqItwulEMFKCwLiKpqKhtoZS7mhRIo4Kgu41/IaHpO/PGd/PJ8qiLMqiLMqiLMqiLO7SGJ/hXhw1X5RFWZTFXXgfL+EhfGoxURZlURYHcAQX8Swm+BGvW0yURVmUxZLGOItVg7fxLW5aTJRFWZTFkh7GSVM7+MLioizKoiyWMMZ5jAxexCXLibIoi7JYwqt4DFfwBnYtL8qiLMpiQd9gBb9gA7sOJsqiLMpiAetYxQRb+MvBRVmURVnMcRhrpn7Djlu9hUcNTpktyqIsymKOf/AM7sE+vjS1gQnexFGDTTyCXbcXZVEWZTHHc1jDPq7jpsEKTuAFgz+wg6dwAS/jmv+KsiiLspjhEJ4w+BUf4yeMcRrruIHP8S4ewFU86M6iLMqiLGY4gfcMPsQZHMFZnMQetrCJJ/EB9nAV19xelEVZlMUMx0ydMbiIVYN1bOM4vjI4h1PuLMqiLMpihsMY4ZLBCh7HCJvYxhjnMcImzpktyqIsymKOCSam9jHBMVzHffgZa/jdfFEWZVEWM3yC01jHcTyNQwavYYQbeAe7FhNlURZlMcPf+BP342tM3GoPW7hicVEWZVEWM3yHV7CB5019hB/wPbYtJ8qiLMpijsu47P8TZVEWZVEWZVEWZVEWZVEWZVEWZVEWZVEWZVEWZVEWZVEWZf8CoKdYWdyPfO8AAAAASUVORK5CYII=",
      "text/plain": [
       "28×28 reinterpret(reshape, Gray{Float32}, adjoint(::Matrix{Float32})) with eltype Gray{Float32}:\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " ⋮                                       ⋱  \n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# l = @layout\n",
    "# plots = \n",
    "# l = @layout [1;5]\n",
    "# a = []\n",
    "# for i in 1:5\n",
    "\n",
    "idx = 14\n",
    "x1 = train_x[:,:,idx];\n",
    "y1 = train_y[idx]\n",
    "println(y1)\n",
    "colorview(Gray, x1', )\n",
    "\n",
    "#     # heatmap(x1, c = :greys, title = y1)\n",
    "#     push!(a, colorview(Gray, x1'))\n",
    "# end\n",
    "# # apply(plot,a)\n",
    "\n",
    "# plot(a[1],a[2], layout = l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 60000)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain = Flux.flatten(train_x);\n",
    "xtest = Flux.flatten(test_x);\n",
    "size(xtrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To type a greek char. Do it like in Word. \n",
    "* \\sigma then hit tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{Flux.OneHotArray{UInt32, 10, 1, 2, Vector{UInt32}}}:\n",
       " [0 1 … 0 0; 0 0 … 0 0; … ; 0 0 … 0 1; 0 0 … 0 0]\n",
       " [0 0 … 0 0; 0 0 … 0 0; … ; 0 0 … 0 0; 0 0 … 0 0]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ytrain, ytest = Flux.onehotbatch(train_y, 0:9)\n",
    "ytrain, ytest = [Flux.onehotbatch(a, 0:9) for a in [train_y, test_y]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [println(\"$s and #n\") for ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = [(a*b) for  a = 0:5,  b =-5:3] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28, 60000)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(m, n, x)  = size(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "σ(x) = 1 / (1 + exp(-x))\n",
       "\\end{verbatim}\n",
       "Classic \\href{https://en.wikipedia.org/wiki/Sigmoid_function}{sigmoid} activation function. Unicode \\texttt{σ} can be entered as \\texttt{{\\textbackslash}sigma} then tab, in many editors. The ascii name \\texttt{sigmoid} is also exported.\n",
       "\n",
       "See also \\href{@ref}{\\texttt{sigmoid\\_fast}}.\n",
       "\n",
       "\\begin{verbatim}\n",
       "julia> using UnicodePlots\n",
       "\n",
       "julia> lineplot(sigmoid, -5, 5, height=7)\n",
       "          ┌────────────────────────────────────────┐     \n",
       "        1 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⣀⡠⠤⠖⠒⠒⠋⠉⠉⠉⠉⠉⠉│ σ(x)\n",
       "          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⢀⡠⠖⠋⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│     \n",
       "          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⣀⠔⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│     \n",
       "   f(x)   │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡠⡏⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│     \n",
       "          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡔⠋⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│     \n",
       "          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠤⠊⠁⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│     \n",
       "        0 │⣀⣀⣀⣀⣀⣀⣀⠤⠤⠤⠒⠊⠉⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│     \n",
       "          └────────────────────────────────────────┘     \n",
       "          ⠀-5⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀5⠀     \n",
       "          ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀     \n",
       "\n",
       "julia> sigmoid === σ\n",
       "true\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "```\n",
       "σ(x) = 1 / (1 + exp(-x))\n",
       "```\n",
       "\n",
       "Classic [sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function) activation function. Unicode `σ` can be entered as `\\sigma` then tab, in many editors. The ascii name `sigmoid` is also exported.\n",
       "\n",
       "See also [`sigmoid_fast`](@ref).\n",
       "\n",
       "```\n",
       "julia> using UnicodePlots\n",
       "\n",
       "julia> lineplot(sigmoid, -5, 5, height=7)\n",
       "          ┌────────────────────────────────────────┐     \n",
       "        1 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⣀⡠⠤⠖⠒⠒⠋⠉⠉⠉⠉⠉⠉│ σ(x)\n",
       "          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⢀⡠⠖⠋⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│     \n",
       "          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⣀⠔⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│     \n",
       "   f(x)   │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡠⡏⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│     \n",
       "          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡔⠋⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│     \n",
       "          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠤⠊⠁⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│     \n",
       "        0 │⣀⣀⣀⣀⣀⣀⣀⠤⠤⠤⠒⠊⠉⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│     \n",
       "          └────────────────────────────────────────┘     \n",
       "          ⠀-5⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀5⠀     \n",
       "          ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀     \n",
       "\n",
       "julia> sigmoid === σ\n",
       "true\n",
       "```\n"
      ],
      "text/plain": [
       "\u001b[36m  σ(x) = 1 / (1 + exp(-x))\u001b[39m\n",
       "\n",
       "  Classic sigmoid (https://en.wikipedia.org/wiki/Sigmoid_function) activation\n",
       "  function. Unicode \u001b[36mσ\u001b[39m can be entered as \u001b[36m\\sigma\u001b[39m then tab, in many editors. The\n",
       "  ascii name \u001b[36msigmoid\u001b[39m is also exported.\n",
       "\n",
       "  See also \u001b[36msigmoid_fast\u001b[39m.\n",
       "\n",
       "\u001b[36m  julia> using UnicodePlots\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> lineplot(sigmoid, -5, 5, height=7)\u001b[39m\n",
       "\u001b[36m            ┌────────────────────────────────────────┐     \u001b[39m\n",
       "\u001b[36m          1 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⣀⡠⠤⠖⠒⠒⠋⠉⠉⠉⠉⠉⠉│ σ(x)\u001b[39m\n",
       "\u001b[36m            │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⢀⡠⠖⠋⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│     \u001b[39m\n",
       "\u001b[36m            │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⣀⠔⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│     \u001b[39m\n",
       "\u001b[36m     f(x)   │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡠⡏⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│     \u001b[39m\n",
       "\u001b[36m            │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡔⠋⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│     \u001b[39m\n",
       "\u001b[36m            │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠤⠊⠁⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│     \u001b[39m\n",
       "\u001b[36m          0 │⣀⣀⣀⣀⣀⣀⣀⠤⠤⠤⠒⠊⠉⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│     \u001b[39m\n",
       "\u001b[36m            └────────────────────────────────────────┘     \u001b[39m\n",
       "\u001b[36m            ⠀-5⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀5⠀     \u001b[39m\n",
       "\u001b[36m            ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀     \u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> sigmoid === σ\u001b[39m\n",
       "\u001b[36m  true\u001b[39m"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?Flux.σ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Dense(784, 60, σ),                    \u001b[90m# 47_100 parameters\u001b[39m\n",
       "  Dense(60, 60, σ),                     \u001b[90m# 3_660 parameters\u001b[39m\n",
       "  Dense(60, 10, σ),                     \u001b[90m# 610 parameters\u001b[39m\n",
       ")\u001b[90m                   # Total: 6 arrays, \u001b[39m51_370 parameters, 201.039 KiB."
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function make_model()\n",
    "    h = 60\n",
    "    in_dims, out_dims = m*n, 10\n",
    "    model = Flux.Chain(\n",
    "        Dense(in_dims, h, Flux.σ), \n",
    "        Dense(h,h, Flux.σ),\n",
    "        Dense(h, out_dims, Flux.σ)\n",
    "\n",
    "    )\n",
    "    return model\n",
    "end\n",
    "model = make_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 1 method)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(x,y) = Flux.Losses.mse(model(x), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?Flux.onecold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accuracy (generic function with 1 method)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Don't need the statisics dot notation. Just helsp to know where it comes from. \n",
    "accuracy(x,y) = Statistics.mean(Flux.onecold(model(x)) .==Flux.onecold(y)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Descent(0.23)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt = Descent(0.23)\n",
    "# opt = Flux.ADAM(0.23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(xtrain, ytrain)];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = Flux.params(model);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "say_status (generic function with 1 method)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function say_status(xtrain, ytrain, state)\n",
    "println(\"$state loss = $(loss(xtrain, ytrain))\")\n",
    "println(\"$state accuracy = $(accuracy(xtrain, ytrain))\\n\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old loss = 0.3207975\n",
      "Old accuracy = 0.09321666666666667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "say_status(xtrain, ytrain, \"Old\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random accuracy is about 0.1 which makes since. 10% of the time you would guesss the correct number out of 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "Flux.train!(loss, parameters, data, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New loss = 0.30246505\n",
      "New accuracy = 0.09385\n",
      "\n"
     ]
    }
   ],
   "source": [
    "say_status(xtrain, ytrain, \"New\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD\n",
    "* previously we used all the data. \n",
    "* Now grab a subset, calc the gradient, step to update the weights, and repeat ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting\n",
      "Training at epoch  = 1000, xtrain size (784, 60000) loss = 0.08938841 and accuracy = 0.12361666666666667\n",
      "Training at epoch  = 2000, xtrain size (784, 60000) loss = 0.08870689 and accuracy = 0.24841666666666667\n",
      "Training at epoch  = 3000, xtrain size (784, 60000) loss = 0.087656304 and accuracy = 0.28253333333333336\n",
      "Training at epoch  = 4000, xtrain size (784, 60000) loss = 0.08567861 and accuracy = 0.32695\n",
      "Training at epoch  = 5000, xtrain size (784, 60000) loss = 0.08157413 and accuracy = 0.40365\n",
      "Training at epoch  = 6000, xtrain size (784, 60000) loss = 0.07553598 and accuracy = 0.5232333333333333\n",
      "Training at epoch  = 7000, xtrain size (784, 60000) loss = 0.06801323 and accuracy = 0.5807166666666667\n",
      "Training at epoch  = 8000, xtrain size (784, 60000) loss = 0.059972897 and accuracy = 0.6378166666666667\n",
      "Training at epoch  = 9000, xtrain size (784, 60000) loss = 0.05361086 and accuracy = 0.6898166666666666\n",
      "Training at epoch  = 10000, xtrain size (784, 60000) loss = 0.048843738 and accuracy = 0.7214333333333334\n"
     ]
    }
   ],
   "source": [
    "# function SGD_Mnist(xtrain, ytrain, opt, epochs_max = 100_000)\n",
    "grad_opt = Descent(0.23)\n",
    "\n",
    "vector_length, num_imgs = size(xtrain)\n",
    "\n",
    "# Make a new model so that we are staring from scratch\n",
    "model = make_model()\n",
    "loss(x,y) = Flux.Losses.mse(model(x), y)\n",
    "accuracy(x,y) = Statistics.mean(Flux.onecold(model(x)) .==Flux.onecold(y)) \n",
    "parameters = Flux.params(model);\n",
    "#     opt = \n",
    "\n",
    "function say_status_V2(xtrain, ytrain, state)\n",
    "    println(\"$state loss = $(loss(xtrain, ytrain)) and accuracy = $(accuracy(xtrain, ytrain))\")\n",
    "end\n",
    "\n",
    "epochs_max=10000\n",
    "say_status_frequency =  Integer(round(epochs_max/10))\n",
    "num_imgs_per_epoch = Integer(round(num_imgs/1000))  # get 0.1% of the data per epoch\n",
    "\n",
    "\n",
    "# loop over `epochs_max` to train the model. \n",
    "println(\"Starting\")\n",
    "for epoch in 1:epochs_max\n",
    "\n",
    "    # randomly select some data. \n",
    "    i = rand(1:num_imgs,num_imgs_per_epoch) \n",
    "   data = [(xtrain[:,i], ytrain[:,i])]    \n",
    "\n",
    "    # Do the training, i.e. updat the weights\n",
    "    Flux.train!(loss, parameters, data, opt)\n",
    "\n",
    "    if epoch % say_status_frequency ==0\n",
    "        say_status_V2(xtrain, ytrain, \"Training at epoch  = $epoch, xtrain size $(size(xtrain))\")\n",
    "#             println(\"Epoch = $epoch\")\n",
    "#             @show loss(xtrain, ytrain)\n",
    "#             @show accuracy(xtrain, ytrain)\n",
    "    end\n",
    "end\n",
    "#     return model\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m2 = SGD_Mnist(xtrain, ytrain, grad_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adam = Flux.ADAM(0.23)\n",
    "# m3 = SGD_Mnist(xtrain, ytrain,  adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#13 (generic function with 1 method)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1 = (x,y) -> x+y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test1(2, 3) = 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@show test1(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Integer(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6-element Vector{Int64}:\n",
       " 1\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5\n",
       " 6"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = collect(1:Integer( round(5.6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25532"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = rand(1:num_imgs-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 10000)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, num_imgs_test = size(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred = 8, actual = 8\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAAgY0hSTQAAeiYAAICEAAD6AAAAgOgAAHUwAADqYAAAOpgAABdwnLpRPAAAAlJJREFUaAW9wU/InwMAB/DPW9+DJcll+df8py2Zw85Llo2y7EDtJmtJbCNGKbWS2sFJSopSitfbWg4YhxEXNGXtQs1LLEO4iGVYXofnsN7Y73nm8P18oizKoizKoizKoizKoizKoizKoizKoizKoizKoizKYqJzsQNrDVbhMuzD1ziCD4yLsiiLsphgHd7ASsvN4UGDv/EuduILZxZlURZlMeI8LGAl/sAr+BTfYDPuxS/4FbfgdVzvzKIsyqIsRjyGy3ES2zHvtKO4Aws4H3djFa7Gov8WZVEWZTFihcFxzFtuEXtwIe43+AGLzizKoizKYsRnBhfgWhy13Pt4DzH4zWxRFmVRFiMWsBUbsAs7LHcDVhr8hQfMFmVRFmUx4gQexQFswznYiy9xBR5GcAoP4SOzRVmURVlMcAS78RLuwQY8iZ1Ya7APzxsXZVEWZTHRPD7BM7gVL2IOS3gb200TZVEWZXEWFvEWbjOYw/d4BCdNE2VRFmVxFjbhcSwZHMaNOIiN+Ny4KIuyKIuJ1uAFXIzjuA8HMY8teAfr8JPZoizKoiwm2oZLcApP4IDB07gdl2I99pstyqIsymKijQav4mWnfYwPsR53Yr/ZoizKoiwmuAZXGZzwb8cM1hgXZVEWZTHBt/gOV1puBW7GVoPDxkVZlEVZTPA73sQubMFqHMJmrDb4EbuNi7Ioi7KY6Flch024CDdhDks4hj342bgoi7Ioi4m+wi7chacMXsMhPIc/TRNlURZlcRYWsRd7/X9RFmVRFmVRFmVRFmVRFmVR9g/z6mPjjtTEBgAAAABJRU5ErkJggg==",
      "text/plain": [
       "28×28 reinterpret(reshape, Gray{Float32}, adjoint(::Matrix{Float32})) with eltype Gray{Float32}:\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " ⋮                                       ⋱  \n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "i = rand(1:num_imgs_test)\n",
    "i\n",
    "\n",
    "predict(i) = argmax(model(xtest[:,i]))-1\n",
    "\n",
    "digit = predict(i)\n",
    "actual =argmax(ytest[:,i])-1\n",
    "println(\"pred = $digit, actual = $actual\")\n",
    "\n",
    "colorview(Gray, test_x[:,:,i]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "using TensorBoardLogger, Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "IOError: unlink(\"content/logs\\\\events.out.tfevents.1.647121539842e9.DESKTOP-7DC3UA9\"): resource busy or locked (EBUSY)",
     "output_type": "error",
     "traceback": [
      "IOError: unlink(\"content/logs\\\\events.out.tfevents.1.647121539842e9.DESKTOP-7DC3UA9\"): resource busy or locked (EBUSY)",
      "",
      "Stacktrace:",
      "  [1] uv_error",
      "    @ .\\libuv.jl:97 [inlined]",
      "  [2] unlink(p::String)",
      "    @ Base.Filesystem .\\file.jl:958",
      "  [3] rm(path::String; force::Bool, recursive::Bool)",
      "    @ Base.Filesystem .\\file.jl:276",
      "  [4] rm(path::String; force::Bool, recursive::Bool)",
      "    @ Base.Filesystem .\\file.jl:287",
      "  [5] init_logdir(logdir::String, overwrite::TensorBoardLogger.InitPolicy)",
      "    @ TensorBoardLogger C:\\Users\\garla\\.julia\\packages\\TensorBoardLogger\\AdBmZ\\src\\TBLogger.jl:70",
      "  [6] TBLogger(logdir::String, overwrite::TensorBoardLogger.InitPolicy; time::Float64, prefix::String, purge_step::Nothing, step_increment::Int64, min_level::Base.CoreLogging.LogLevel)",
      "    @ TensorBoardLogger C:\\Users\\garla\\.julia\\packages\\TensorBoardLogger\\AdBmZ\\src\\TBLogger.jl:49",
      "  [7] TBLogger(logdir::String, overwrite::TensorBoardLogger.InitPolicy)",
      "    @ TensorBoardLogger C:\\Users\\garla\\.julia\\packages\\TensorBoardLogger\\AdBmZ\\src\\TBLogger.jl:49",
      "  [8] top-level scope",
      "    @ In[145]:1",
      "  [9] eval",
      "    @ .\\boot.jl:373 [inlined]",
      " [10] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "    @ Base .\\loading.jl:1196"
     ]
    }
   ],
   "source": [
    "logger = TBLogger(\"content/logs\", tb_overwrite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_logger(logger) do \n",
    "   images = TBImage(train_x[:,:,1:10],WHN)\n",
    "    @info \"mnist/samples\" pics = images log_step_increment = 0\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here\n",
    "https://github.com/JuliaLogging/TensorBoardLogger.jl/blob/master/examples/Flux.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fill_param_dict! (generic function with 1 method)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#function to get dictionary of model parameters\n",
    "function fill_param_dict!(dict, m, prefix)\n",
    "    if m isa Chain\n",
    "        for (i, layer) in enumerate(m.layers)\n",
    "            fill_param_dict!(dict, layer, prefix*\"layer_\"*string(i)*\"/\"*string(layer)*\"/\")\n",
    "        end\n",
    "    else\n",
    "        for fieldname in fieldnames(typeof(m))\n",
    "            val = getfield(m, fieldname)\n",
    "            if val isa AbstractArray\n",
    "                val = vec(val)\n",
    "            end\n",
    "            dict[prefix*string(fieldname)] = val\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TBCallback (generic function with 1 method)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#function to log information after every epoch\n",
    "function TBCallback()\n",
    "  param_dict = Dict{String, Any}()\n",
    "  fill_param_dict!(param_dict, model, \"\")\n",
    "  with_logger(logger) do\n",
    "    @info \"model\" params=param_dict log_step_increment=0\n",
    "    @info \"train\" loss=loss(xtrain, ytrain) acc=accuracy(xtrain, ytrain) log_step_increment=0\n",
    "    @info \"test\" loss=loss(xtest, ytest) acc=accuracy(xtest, ytest)\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "ADAM(η = 0.001, β::Tuple = (0.9, 0.999), ϵ = 1.0e-8)\n",
       "\\end{verbatim}\n",
       "\\href{https://arxiv.org/abs/1412.6980}{ADAM} optimiser.\n",
       "\n",
       "\\section{Parameters}\n",
       "\\begin{itemize}\n",
       "\\item Learning rate (\\texttt{η}): Amount by which gradients are discounted before updating                      the weights.\n",
       "\n",
       "\n",
       "\\item Decay of momentums (\\texttt{β::Tuple}): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.\n",
       "\n",
       "\\end{itemize}\n",
       "\\section{Examples}\n",
       "\\begin{verbatim}\n",
       "opt = ADAM()\n",
       "\n",
       "opt = ADAM(0.001, (0.9, 0.8))\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "```\n",
       "ADAM(η = 0.001, β::Tuple = (0.9, 0.999), ϵ = 1.0e-8)\n",
       "```\n",
       "\n",
       "[ADAM](https://arxiv.org/abs/1412.6980) optimiser.\n",
       "\n",
       "# Parameters\n",
       "\n",
       "  * Learning rate (`η`): Amount by which gradients are discounted before updating                      the weights.\n",
       "  * Decay of momentums (`β::Tuple`): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.\n",
       "\n",
       "# Examples\n",
       "\n",
       "```julia\n",
       "opt = ADAM()\n",
       "\n",
       "opt = ADAM(0.001, (0.9, 0.8))\n",
       "```\n"
      ],
      "text/plain": [
       "\u001b[36m  ADAM(η = 0.001, β::Tuple = (0.9, 0.999), ϵ = 1.0e-8)\u001b[39m\n",
       "\n",
       "  ADAM (https://arxiv.org/abs/1412.6980) optimiser.\n",
       "\n",
       "\u001b[1m  Parameters\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "    •  Learning rate (\u001b[36mη\u001b[39m): Amount by which gradients are discounted before\n",
       "       updating the weights.\n",
       "\n",
       "    •  Decay of momentums (\u001b[36mβ::Tuple\u001b[39m): Exponential decay for the first\n",
       "       (β1) and the second (β2) momentum estimate.\n",
       "\n",
       "\u001b[1m  Examples\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[36m  opt = ADAM()\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  opt = ADAM(0.001, (0.9, 0.8))\u001b[39m"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "? Flux.ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function SGD_Mnist(xtrain, ytrain, opt, epochs_max = 100_000)\n",
    "grad_opt =  Flux.ADAM(0.23)\n",
    "\n",
    "vector_length, num_imgs = size(xtrain)\n",
    "\n",
    "# Make a new model so that we are staring from scratch\n",
    "model = make_model()\n",
    "loss(x,y) = Flux.Losses.mse(model(x), y)\n",
    "accuracy(x,y) = Statistics.mean(Flux.onecold(model(x)) .==Flux.onecold(y)) \n",
    "parameters = Flux.params(model);\n",
    "#     opt = \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "say_status_V2 (generic function with 2 methods)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs_max=100\n",
    "say_status_frequency =  Integer(round(epochs_max/10))\n",
    "num_imgs_per_epoch = Integer(round(num_imgs/1000))  # get 0.1% of the data per epoch\n",
    "\n",
    "m = model\n",
    "l = loss\n",
    "acc = accuracy\n",
    "p = parameters\n",
    "d  = data;\n",
    "\n",
    "function say_status_V2( state)\n",
    "    println(\"$state loss = $(loss(xtrain, ytrain)) and accuracy = $(accuracy(xtrain, ytrain))\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs_max=200\n",
    "# it does seem much slower!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting\n",
      "Training at epoch  = 10, xtrain size (784, 60000) loss = 0.091326654 and accuracy = 0.11251666666666667\n",
      "Training at epoch  = 20, xtrain size (784, 60000) loss = 0.09133341 and accuracy = 0.11251666666666667\n",
      "Training at epoch  = 30, xtrain size (784, 60000) loss = 0.09134228 and accuracy = 0.11253333333333333\n",
      "Training at epoch  = 40, xtrain size (784, 60000) loss = 0.09135264 and accuracy = 0.11253333333333333\n",
      "Training at epoch  = 50, xtrain size (784, 60000) loss = 0.09136403 and accuracy = 0.11253333333333333\n",
      "Training at epoch  = 60, xtrain size (784, 60000) loss = 0.09137605 and accuracy = 0.11255\n",
      "Training at epoch  = 70, xtrain size (784, 60000) loss = 0.091388434 and accuracy = 0.11255\n",
      "Training at epoch  = 80, xtrain size (784, 60000) loss = 0.09140095 and accuracy = 0.11255\n",
      "Training at epoch  = 90, xtrain size (784, 60000) loss = 0.09141344 and accuracy = 0.11256666666666666\n",
      "Training at epoch  = 100, xtrain size (784, 60000) loss = 0.091425754 and accuracy = 0.11261666666666667\n",
      "Training at epoch  = 110, xtrain size (784, 60000) loss = 0.09143783 and accuracy = 0.11266666666666666\n",
      "Training at epoch  = 120, xtrain size (784, 60000) loss = 0.091449566 and accuracy = 0.11268333333333333\n",
      "Training at epoch  = 130, xtrain size (784, 60000) loss = 0.09146092 and accuracy = 0.11271666666666667\n",
      "Training at epoch  = 140, xtrain size (784, 60000) loss = 0.09147186 and accuracy = 0.11276666666666667\n",
      "Training at epoch  = 150, xtrain size (784, 60000) loss = 0.09148234 and accuracy = 0.11283333333333333\n",
      "Training at epoch  = 160, xtrain size (784, 60000) loss = 0.09149237 and accuracy = 0.11291666666666667\n",
      "Training at epoch  = 170, xtrain size (784, 60000) loss = 0.09150192 and accuracy = 0.11311666666666667\n",
      "Training at epoch  = 180, xtrain size (784, 60000) loss = 0.091511 and accuracy = 0.11336666666666667\n",
      "Training at epoch  = 190, xtrain size (784, 60000) loss = 0.09151961 and accuracy = 0.1136\n",
      "Training at epoch  = 200, xtrain size (784, 60000) loss = 0.09152775 and accuracy = 0.11386666666666667\n",
      "Training at epoch  = 210, xtrain size (784, 60000) loss = 0.09153543 and accuracy = 0.11411666666666667\n",
      "Training at epoch  = 220, xtrain size (784, 60000) loss = 0.09154266 and accuracy = 0.11445\n",
      "Training at epoch  = 230, xtrain size (784, 60000) loss = 0.09154946 and accuracy = 0.11473333333333334\n",
      "Training at epoch  = 240, xtrain size (784, 60000) loss = 0.091555834 and accuracy = 0.11513333333333334\n",
      "Training at epoch  = 250, xtrain size (784, 60000) loss = 0.09156179 and accuracy = 0.11568333333333333\n",
      "Training at epoch  = 260, xtrain size (784, 60000) loss = 0.09156734 and accuracy = 0.11611666666666667\n",
      "Training at epoch  = 270, xtrain size (784, 60000) loss = 0.09157251 and accuracy = 0.11648333333333333\n",
      "Training at epoch  = 280, xtrain size (784, 60000) loss = 0.09157731 and accuracy = 0.1171\n",
      "Training at epoch  = 290, xtrain size (784, 60000) loss = 0.09158173 and accuracy = 0.11773333333333333\n",
      "Training at epoch  = 300, xtrain size (784, 60000) loss = 0.091585815 and accuracy = 0.11838333333333333\n",
      "Training at epoch  = 310, xtrain size (784, 60000) loss = 0.091589555 and accuracy = 0.11918333333333334\n",
      "Training at epoch  = 320, xtrain size (784, 60000) loss = 0.09159298 and accuracy = 0.11991666666666667\n",
      "Training at epoch  = 330, xtrain size (784, 60000) loss = 0.0915961 and accuracy = 0.1208\n",
      "Training at epoch  = 340, xtrain size (784, 60000) loss = 0.09159891 and accuracy = 0.12168333333333334\n",
      "Training at epoch  = 350, xtrain size (784, 60000) loss = 0.09160143 and accuracy = 0.12245\n",
      "Training at epoch  = 360, xtrain size (784, 60000) loss = 0.09160368 and accuracy = 0.1233\n",
      "Training at epoch  = 370, xtrain size (784, 60000) loss = 0.091605656 and accuracy = 0.12426666666666666\n",
      "Training at epoch  = 380, xtrain size (784, 60000) loss = 0.091607384 and accuracy = 0.12528333333333333\n",
      "Training at epoch  = 390, xtrain size (784, 60000) loss = 0.091608845 and accuracy = 0.12641666666666668\n",
      "Training at epoch  = 400, xtrain size (784, 60000) loss = 0.09161008 and accuracy = 0.12745\n",
      "Training at epoch  = 410, xtrain size (784, 60000) loss = 0.091611065 and accuracy = 0.12853333333333333\n",
      "Training at epoch  = 420, xtrain size (784, 60000) loss = 0.09161185 and accuracy = 0.1296\n",
      "Training at epoch  = 430, xtrain size (784, 60000) loss = 0.0916124 and accuracy = 0.13061666666666666\n",
      "Training at epoch  = 440, xtrain size (784, 60000) loss = 0.09161275 and accuracy = 0.1319\n",
      "Training at epoch  = 450, xtrain size (784, 60000) loss = 0.091612905 and accuracy = 0.13323333333333334\n",
      "Training at epoch  = 460, xtrain size (784, 60000) loss = 0.09161285 and accuracy = 0.13448333333333334\n",
      "Training at epoch  = 470, xtrain size (784, 60000) loss = 0.091612615 and accuracy = 0.1357\n",
      "Training at epoch  = 480, xtrain size (784, 60000) loss = 0.0916122 and accuracy = 0.13696666666666665\n",
      "Training at epoch  = 490, xtrain size (784, 60000) loss = 0.091611594 and accuracy = 0.13823333333333335\n",
      "Training at epoch  = 500, xtrain size (784, 60000) loss = 0.091610834 and accuracy = 0.13965\n",
      "Training at epoch  = 510, xtrain size (784, 60000) loss = 0.091609895 and accuracy = 0.14091666666666666\n",
      "Training at epoch  = 520, xtrain size (784, 60000) loss = 0.0916088 and accuracy = 0.14273333333333332\n",
      "Training at epoch  = 530, xtrain size (784, 60000) loss = 0.091607556 and accuracy = 0.14428333333333335\n",
      "Training at epoch  = 540, xtrain size (784, 60000) loss = 0.09160615 and accuracy = 0.14556666666666668\n",
      "Training at epoch  = 550, xtrain size (784, 60000) loss = 0.0916046 and accuracy = 0.14715\n",
      "Training at epoch  = 560, xtrain size (784, 60000) loss = 0.09160289 and accuracy = 0.14825\n",
      "Training at epoch  = 570, xtrain size (784, 60000) loss = 0.09160105 and accuracy = 0.14966666666666667\n",
      "Training at epoch  = 580, xtrain size (784, 60000) loss = 0.09159908 and accuracy = 0.15135\n",
      "Training at epoch  = 590, xtrain size (784, 60000) loss = 0.091596976 and accuracy = 0.15283333333333332\n",
      "Training at epoch  = 600, xtrain size (784, 60000) loss = 0.091594726 and accuracy = 0.15445\n",
      "Training at epoch  = 610, xtrain size (784, 60000) loss = 0.091592364 and accuracy = 0.1557\n",
      "Training at epoch  = 620, xtrain size (784, 60000) loss = 0.09158986 and accuracy = 0.15701666666666667\n",
      "Training at epoch  = 630, xtrain size (784, 60000) loss = 0.09158725 and accuracy = 0.15838333333333332\n",
      "Training at epoch  = 640, xtrain size (784, 60000) loss = 0.09158451 and accuracy = 0.15991666666666668\n",
      "Training at epoch  = 650, xtrain size (784, 60000) loss = 0.09158165 and accuracy = 0.16123333333333334\n",
      "Training at epoch  = 660, xtrain size (784, 60000) loss = 0.091578685 and accuracy = 0.16271666666666668\n",
      "Training at epoch  = 670, xtrain size (784, 60000) loss = 0.0915756 and accuracy = 0.16423333333333334\n",
      "Training at epoch  = 680, xtrain size (784, 60000) loss = 0.09157241 and accuracy = 0.16575\n",
      "Training at epoch  = 690, xtrain size (784, 60000) loss = 0.091569096 and accuracy = 0.1671\n",
      "Training at epoch  = 700, xtrain size (784, 60000) loss = 0.09156568 and accuracy = 0.16888333333333333\n",
      "Training at epoch  = 710, xtrain size (784, 60000) loss = 0.09156216 and accuracy = 0.17033333333333334\n",
      "Training at epoch  = 720, xtrain size (784, 60000) loss = 0.09155854 and accuracy = 0.17185\n",
      "Training at epoch  = 730, xtrain size (784, 60000) loss = 0.091554806 and accuracy = 0.1735\n",
      "Training at epoch  = 740, xtrain size (784, 60000) loss = 0.09155097 and accuracy = 0.17506666666666668\n",
      "Training at epoch  = 750, xtrain size (784, 60000) loss = 0.091547035 and accuracy = 0.17638333333333334\n",
      "Training at epoch  = 760, xtrain size (784, 60000) loss = 0.091543 and accuracy = 0.17816666666666667\n",
      "Training at epoch  = 770, xtrain size (784, 60000) loss = 0.091538854 and accuracy = 0.17955\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "InterruptException:",
     "output_type": "error",
     "traceback": [
      "InterruptException:",
      "",
      "Stacktrace:",
      "  [1] try_yieldto(undo::typeof(Base.ensure_rescheduled))",
      "    @ Base .\\task.jl:777",
      "  [2] wait()",
      "    @ Base .\\task.jl:837",
      "  [3] yield()",
      "    @ Base .\\task.jl:721",
      "  [4] throttled",
      "    @ C:\\Users\\garla\\.julia\\packages\\Flux\\qAdFM\\src\\utils.jl:705 [inlined]",
      "  [5] throttled",
      "    @ C:\\Users\\garla\\.julia\\packages\\Flux\\qAdFM\\src\\utils.jl:705 [inlined]",
      "  [6] macro expansion",
      "    @ C:\\Users\\garla\\.julia\\packages\\Flux\\qAdFM\\src\\optimise\\train.jl:113 [inlined]",
      "  [7] macro expansion",
      "    @ C:\\Users\\garla\\.julia\\packages\\Juno\\n6wyj\\src\\progress.jl:134 [inlined]",
      "  [8] train!(loss::Function, ps::Zygote.Params, data::Vector{Tuple{Matrix{Float32}, Flux.OneHotArray{UInt32, 10, 1, 2, Vector{UInt32}}}}, opt::Descent; cb::Flux.var\"#throttled#74\"{Flux.var\"#throttled#70#75\"{Bool, Bool, typeof(TBCallback), Int64}})",
      "    @ Flux.Optimise C:\\Users\\garla\\.julia\\packages\\Flux\\qAdFM\\src\\optimise\\train.jl:107",
      "  [9] top-level scope",
      "    @ .\\In[161]:11",
      " [10] eval",
      "    @ .\\boot.jl:373 [inlined]",
      " [11] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "    @ Base .\\loading.jl:1196"
     ]
    }
   ],
   "source": [
    "\n",
    "# loop over `epochs_max` to train the model. \n",
    "println(\"Starting\")\n",
    "for epoch in 1:epochs_max\n",
    "\n",
    "    # randomly select some data. \n",
    "    i = rand(1:num_imgs,num_imgs_per_epoch) \n",
    "   data = [(xtrain[:,i], ytrain[:,i])]    \n",
    "\n",
    "    # Do the training, i.e. updat the weights\n",
    "    Flux.train!(l, p, d, opt, cb = Flux.throttle(TBCallback, 5))\n",
    "\n",
    "    if epoch % say_status_frequency ==0\n",
    "        say_status_V2(  \"Training at epoch  = $epoch, xtrain size $(size(xtrain))\")\n",
    "#          println(\"$state loss = $(l(xtrain, ytrain)) and accuracy = $(acc(xtrain, ytrain))\")\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training at epoch  = finished, xtrain size (784, 60000) loss = 0.091536745 and accuracy = 0.18028333333333332\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "InterruptException:",
     "output_type": "error",
     "traceback": [
      "InterruptException:",
      "",
      "Stacktrace:",
      "  [1] try_yieldto(undo::typeof(Base.ensure_rescheduled))",
      "    @ Base .\\task.jl:777",
      "  [2] wait()",
      "    @ Base .\\task.jl:837",
      "  [3] uv_write(s::Base.PipeEndpoint, p::Ptr{UInt8}, n::UInt64)",
      "    @ Base .\\stream.jl:992",
      "  [4] unsafe_write(s::Base.PipeEndpoint, p::Ptr{UInt8}, n::UInt64)",
      "    @ Base .\\stream.jl:1064",
      "  [5] unsafe_write",
      "    @ .\\io.jl:362 [inlined]",
      "  [6] write",
      "    @ .\\strings\\io.jl:244 [inlined]",
      "  [7] print",
      "    @ .\\strings\\io.jl:246 [inlined]",
      "  [8] print(::IJulia.IJuliaStdio{Base.PipeEndpoint}, ::String, ::String)",
      "    @ Base .\\strings\\io.jl:46",
      "  [9] println(io::IJulia.IJuliaStdio{Base.PipeEndpoint}, xs::String)",
      "    @ Base .\\strings\\io.jl:75",
      " [10] println(xs::String)",
      "    @ Base .\\coreio.jl:4",
      " [11] say_status_V2(state::String)",
      "    @ Main .\\In[151]:12",
      " [12] top-level scope",
      "    @ In[162]:2",
      " [13] eval",
      "    @ .\\boot.jl:373 [inlined]",
      " [14] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "    @ Base .\\loading.jl:1196"
     ]
    }
   ],
   "source": [
    "epoch = \"finished\"\n",
    " say_status_V2(  \"Training at epoch  = $epoch, xtrain size $(size(xtrain))\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
